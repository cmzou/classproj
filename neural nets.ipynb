{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "from nltk.corpus import stopwords # get stopwords to remove\n",
    "import re # regular expression\n",
    "from gensim.models import doc2vec # for word embeddings\n",
    "from gensim.utils import simple_preprocess # to tokenize automatically\n",
    "from sklearn.model_selection import train_test_split, KFold # for test-train split & cross validation\n",
    "from sklearn.preprocessing import MultiLabelBinarizer # to convert to a format that can do multi-label classification\n",
    "import random\n",
    "import keras # for nn\n",
    "import tensorflow as tf # for nn & new loss\n",
    "import keras.backend.tensorflow_backend as tfb # for nn $ new loss\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "# Ensure reproducibility\n",
    "seed = 561\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "vec_dim = 100 # how big the word embeddings are"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-label classification\n",
    "\n",
    "Classify the reasons (violations) behind each docket_num (document)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text = pd.read_csv('./data/clean_mea_text.csv') # this holds the raw text\n",
    "reasons = pd.read_csv('./data/mea_reasons_filtered.csv') # these are our target classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(raw_text.head())\n",
    "print(raw_text.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(reasons.head())\n",
    "print(reasons.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# There is a bit of data mismatch, so filter both dfs for text that appears in both\n",
    "bothdocs = set(raw_text.docket_num.values).intersection(reasons.docket_num.values)\n",
    "raw_text = raw_text[raw_text.docket_num.isin(bothdocs)]\n",
    "reasons = reasons[reasons.docket_num.isin(bothdocs)]\n",
    "print(raw_text.shape)\n",
    "print(reasons.shape)\n",
    "\n",
    "# Also need to convert datatypes to prevent type mismatch\n",
    "raw_text['text'] = raw_text['text'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove irrelevant/unhelpful labels\n",
    "toremove = ['Unknown', \n",
    "           'Other', \n",
    "           'Did not appear for the Hearing', \n",
    "           'Other failure to disclose', \n",
    "           'Unknown failure to disclose', \n",
    "           'Allowed falsification and misrepresentation of loans',\n",
    "           'Circumvented the requirements that a branch manager of a licensed mortgage broker have at least three years experience',\n",
    "           \"Did not verify or make a reasonable effort to verify the borrower's information\",\n",
    "           'Employed simultaneously by more than one affiliated mortgage banker or mortgage broker',\n",
    "           'Engaged in fraud',\n",
    "           'Failure to disclose charges',\n",
    "           'Violated NC Securities Act',\n",
    "           'Withdrew appeal']\n",
    "reasons = reasons[~reasons.reason.isin(toremove)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we want to do multi-label classification, binarize outputs\n",
    "# First, need to aggregate reason by docket_num\n",
    "reasonsls = reasons.groupby('docket_num')['reason'].apply(set).reset_index(name='reason')\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "classesbin = mlb.fit_transform(reasonsls.reason.values)\n",
    "classesbin = pd.DataFrame(classesbin)\n",
    "classesbin.columns = mlb.classes_\n",
    "\n",
    "reasonsls = pd.concat([reasonsls, classesbin], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's combine the input and output datasets for easier handling\n",
    "merged = raw_text.merge(reasonsls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Params:\n",
    "#    df - dataframe with column 'text' to be tokenized\n",
    "#    tokens_only - to train the doc2vec model, weâ€™ll need to \n",
    "#        associate a tag/number with each document of the training corpus. \n",
    "#        tokens_only=True means don't associate anything\n",
    "def tokenize(df, tokens_only=False):\n",
    "    tokens = df['text'].apply(lambda x: simple_preprocess(x, deacc=True, max_len=20)) # max_len=20 just in case there are important words 15 chars long)\n",
    "    if tokens_only:\n",
    "        return tokens\n",
    "    else:\n",
    "        # For training data, add tags -- notice it is just an index number\n",
    "        return [doc2vec.TaggedDocument(doc, [i]) for i, doc in enumerate(tokens)]\n",
    "\n",
    "corpus = tokenize(merged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorize\n",
    "\n",
    "We use the Continuous Bag of Words (CBOW) model to create our word embeddings to be used in our ML models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wordmodel = doc2vec.Doc2Vec(vector_size=vec_dim,min_count=1, epochs=40) # min_count=1 because we're not sure if relevant words occur multiple times\n",
    "wordmodel.build_vocab(corpus)\n",
    "# Train\n",
    "wordmodel.train(corpus, total_examples=wordmodel.corpus_count, epochs=wordmodel.epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assess CBOW model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks = []\n",
    "second_ranks = []\n",
    "docvecs = []\n",
    "for doc_id in range(len(corpus)):\n",
    "    inferred_vector = wordmodel.infer_vector(corpus[doc_id].words)\n",
    "    sims = wordmodel.docvecs.most_similar([inferred_vector], topn=len(wordmodel.docvecs))\n",
    "    rank = [docid for docid, sim in sims].index(doc_id)\n",
    "    ranks.append(rank)\n",
    "\n",
    "    second_ranks.append(sims[1])\n",
    "    \n",
    "    docvecs.append(inferred_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "counter = collections.Counter(ranks)\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add doc embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "merged = pd.concat([merged, pd.DataFrame(docvecs)], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train & Test Neural Network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_dim = len(mlb.classes_) # number of distinct classes\n",
    "epochs = 50\n",
    "batch_size = 10\n",
    "k = 5 # number of folds for cv\n",
    "pweights = [15, 10, 5, 4, 3, 2, 1]  # multiplier for positive targets, needs to be tuned\n",
    "thresholds = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6] # thresholds for 1/0, needs to be tuned\n",
    "\n",
    "pos_weight = pweights[0] # starting weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From https://stackoverflow.com/questions/42158866/neural-network-for-multi-label-classification-with-large-number-of-classes-outpu/47313183#47313183\n",
    "def weighted_binary_crossentropy(target, output):\n",
    "    \"\"\"\n",
    "    Weighted binary crossentropy between an output tensor \n",
    "    and a target tensor. POS_WEIGHT is used as a multiplier \n",
    "    for the positive targets.\n",
    "\n",
    "    Combination of the following functions:\n",
    "    * keras.losses.binary_crossentropy\n",
    "    * keras.backend.tensorflow_backend.binary_crossentropy\n",
    "    * tf.nn.weighted_cross_entropy_with_logits\n",
    "    \"\"\"\n",
    "    # transform back to logits\n",
    "    _epsilon = tfb._to_tensor(tfb.epsilon(), output.dtype.base_dtype)\n",
    "    output = tf.clip_by_value(output, _epsilon, 1 - _epsilon)\n",
    "    output = tf.log(output / (1 - output))\n",
    "    # compute weighted loss\n",
    "    loss = tf.nn.weighted_cross_entropy_with_logits(targets=target,\n",
    "                                                    logits=output,\n",
    "                                                    pos_weight=pos_weight)\n",
    "    return tf.reduce_mean(loss, axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train & test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into x and y\n",
    "x = merged.iloc[:, 7+class_dim:]\n",
    "y = merged.iloc[:, 7:7+class_dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Cross validation\n",
    "kf = KFold(n_splits=k, shuffle=True)\n",
    "\n",
    "cv_scores = [] # note to self: change into np array\n",
    "for weight in pweights:\n",
    "    pos_weight = weight\n",
    "    for train_index, test_index in kf.split(x):\n",
    "        X_train, X_test = x.iloc[train_index], x.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        # Build model\n",
    "        model = keras.models.Sequential() \n",
    "        model.add(keras.layers.Dense(128, input_dim=vec_dim, activation='relu'))\n",
    "        model.add(keras.layers.Dense(64, activation='relu'))\n",
    "        model.add(keras.layers.Dense(class_dim, activation='sigmoid')) # sigmoid so that the probability of one class is independent from the probability of another\n",
    "        model.compile(loss=weighted_binary_crossentropy, \n",
    "                  optimizer='adam', \n",
    "                  metrics=['accuracy'])\n",
    "        # Fit model\n",
    "        model.fit(X_train, \n",
    "                     y_train,\n",
    "                     epochs=epochs,\n",
    "                     batch_size=batch_size)\n",
    "        \n",
    "        # Evaluate\n",
    "        loss, acc = model.evaluate(X_test, y_test)\n",
    "#         # Predict\n",
    "#         y_pred = model.predict(X_test)\n",
    "#         y_pred[y_pred>=0.5] = 1\n",
    "#         y_pred[y_pred<0.5] = 0\n",
    "\n",
    "        cv_scores.append(acc)\n",
    "print(cv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = np.array(cv_scores)\n",
    "for i in range(len(pweights)):\n",
    "    print('For pos_weight={:d}, accuracy={:.4f}'.format(pweights[i], temp[i*5:(i*5+k)].mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
