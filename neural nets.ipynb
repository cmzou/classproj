{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "unable to import 'smart_open.gcs', disabling that module\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "from nltk.corpus import stopwords # get stopwords to remove\n",
    "import re # regular expression\n",
    "from gensim.models import doc2vec, Word2Vec # for word embeddings\n",
    "from gensim.utils import simple_preprocess # to tokenize automatically\n",
    "from sklearn.model_selection import train_test_split, KFold # for test-train split & cross validation\n",
    "from sklearn.preprocessing import MultiLabelBinarizer # to convert to a format that can do multi-label classification\n",
    "import random\n",
    "import keras # for nn\n",
    "import tensorflow as tf # for nn & new loss\n",
    "import keras.backend.tensorflow_backend as tfb # for nn $ new loss\n",
    "from sklearn.metrics import precision_score, accuracy_score, roc_auc_score, f1_score # for metrics\n",
    "import collections\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "# Ensure reproducibility\n",
    "seed = 561\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "tf.set_random_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "vec_dim = 100 # how big the word embeddings are"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-label classification\n",
    "\n",
    "Classify the reasons (violations) behind each docket_num (document)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text = pd.read_csv('./data/clean_mea_text.csv') # this holds the raw text\n",
    "reasons = pd.read_csv('./data/mea_reasons_filtered.csv') # these are our target classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         date docket_num                                               text  \\\n",
      "0  2009-11-18     09_160  STATE OF NORTH CAROLINA\\nWAKE COUNTY\\nIN A MAT...   \n",
      "1  2009-11-18     09_164  STATE OF NORTH CAROLINA\\nWAKE COUNTY\\nIN A MAT...   \n",
      "2  2009-10-16    09_142B  OAH File No. 10 COB 2895\\nSTATE OF NORTH CAROL...   \n",
      "3  2009-09-09     09_081  STATE OF NORTH CAROLINA\\nWAKE COUNTY\\nIN A MAT...   \n",
      "4  2009-08-24     09_070  STATE OF NORTH CAROLINA\\nWAKE COUNTY\\nIN A MAT...   \n",
      "\n",
      "   year  month  day  \n",
      "0  2009     11   18  \n",
      "1  2009     11   18  \n",
      "2  2009     10   16  \n",
      "3  2009      9    9  \n",
      "4  2009      8   24  \n",
      "(177, 6)\n"
     ]
    }
   ],
   "source": [
    "print(raw_text.head())\n",
    "print(raw_text.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  docket_num        date                                        reason\n",
      "0     09_160  11/18/2009                    Conspiracy to commit fraud\n",
      "1     09_164  11/18/2009                    Conspiracy to commit fraud\n",
      "2    09_142B  10/16/2009                     Allowed unlawful activity\n",
      "3     09_081    9/9/2009  Falsification and misrepresentation of loans\n",
      "4     09_070   8/24/2009                       Retained borrower funds\n",
      "(375, 3)\n"
     ]
    }
   ],
   "source": [
    "print(reasons.head())\n",
    "print(reasons.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(169, 6)\n",
      "(359, 3)\n"
     ]
    }
   ],
   "source": [
    "# There is a bit of data mismatch, so filter both dfs for text that appears in both\n",
    "bothdocs = set(raw_text.docket_num.values).intersection(reasons.docket_num.values)\n",
    "raw_text = raw_text[raw_text.docket_num.isin(bothdocs)]\n",
    "reasons = reasons[reasons.docket_num.isin(bothdocs)]\n",
    "print(raw_text.shape)\n",
    "print(reasons.shape)\n",
    "\n",
    "# Also need to convert datatypes to prevent type mismatch\n",
    "raw_text['text'] = raw_text['text'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove irrelevant/unhelpful labels\n",
    "toremove = ['Unknown', \n",
    "           'Other', \n",
    "           'Did not appear for the Hearing', \n",
    "           'Other failure to disclose', \n",
    "           'Unknown failure to disclose', \n",
    "           'Allowed falsification and misrepresentation of loans',\n",
    "           'Circumvented the requirements that a branch manager of a licensed mortgage broker have at least three years experience',\n",
    "           \"Did not verify or make a reasonable effort to verify the borrower's information\",\n",
    "           'Employed simultaneously by more than one affiliated mortgage banker or mortgage broker',\n",
    "           'Engaged in fraud',\n",
    "           'Failure to disclose charges',\n",
    "           'Violated NC Securities Act',\n",
    "           'Withdrew appeal',\n",
    "           'Unsatisfactory credit']\n",
    "reasons = reasons[~reasons.reason.isin(toremove)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we want to do multi-label classification, binarize outputs\n",
    "# First, need to aggregate reason by docket_num\n",
    "reasonsls = reasons.groupby('docket_num')['reason'].apply(set).reset_index(name='reason')\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "classesbin = mlb.fit_transform(reasonsls.reason.values)\n",
    "classesbin = pd.DataFrame(classesbin)\n",
    "classesbin.columns = mlb.classes_\n",
    "\n",
    "reasonsls = pd.concat([reasonsls, classesbin], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's combine the input and output datasets for easier handling\n",
    "merged = raw_text.merge(reasonsls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(153, 25)\n"
     ]
    }
   ],
   "source": [
    "print(merged.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Params:\n",
    "#    df - dataframe with column 'text' to be tokenized\n",
    "#    tokens_only - to train the doc2vec model, weâ€™ll need to \n",
    "#        associate a tag/number with each document of the training corpus. \n",
    "#        tokens_only=True means don't associate anything\n",
    "def tokenize(df, tokens_only=False):\n",
    "    tokens = df['text'].apply(lambda x: simple_preprocess(x, deacc=True, max_len=20)) # max_len=20 just in case there are important words 15 chars long)\n",
    "    if tokens_only:\n",
    "        return tokens\n",
    "    else:\n",
    "        # For training data, add tags -- notice it is just an index number\n",
    "        return [doc2vec.TaggedDocument(doc, [i]) for i, doc in enumerate(tokens)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorize\n",
    "\n",
    "We use the Continuous Bag of Words (CBOW) model to create our word embeddings to be used in our ML models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Remove extra space\n",
    "merged['text'] = merged['text'].apply(lambda x: re.sub(r'\\s+', ' ', x))\n",
    "# Tokenize\n",
    "corpus = tokenize(merged)\n",
    "\n",
    "docmodel = doc2vec.Doc2Vec(vector_size=vec_dim,min_count=1, epochs=40) # min_count=1 because we're not sure if relevant words occur multiple times\n",
    "docmodel.build_vocab(corpus)\n",
    "# Train\n",
    "docmodel.train(corpus, total_examples=docmodel.corpus_count, epochs=docmodel.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Phoenix\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.vectors instead).\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "# Let's also use word2vec and averaging to compare to doc2vec\n",
    "# Perhaps in this setting, the context of words isn't important\n",
    "textls = tokenize(merged, tokens_only=True)\n",
    "wordmodel = Word2Vec(textls, min_count=1)\n",
    "w2v = dict(zip(wordmodel.wv.index2word, wordmodel.wv.syn0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Params:\n",
    "#    w2v - dictionary of words to vectors\n",
    "#    text - list of tokenized words to convert to vector\n",
    "# Returns a vector resulting from the average of all present words in text\n",
    "def average_vectors(w2v, text):\n",
    "    num_count = collections.Counter(text) # words and their counts\n",
    "    return np.mean([w2v[word]*count for word, count in num_count.items()], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "docvecs_avg = [average_vectors(w2v, corpus[i].words) for i in range(len(corpus))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assess Doc2Vec CBOW model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks = []\n",
    "second_ranks = []\n",
    "docvecs = []\n",
    "for doc_id in range(len(corpus)):\n",
    "    inferred_vector = docmodel.infer_vector(corpus[doc_id].words)\n",
    "    sims = docmodel.docvecs.most_similar([inferred_vector], topn=len(docmodel.docvecs))\n",
    "    rank = [docid for docid, sim in sims].index(doc_id)\n",
    "    ranks.append(rank)\n",
    "\n",
    "    second_ranks.append(sims[1])\n",
    "    \n",
    "    docvecs.append(inferred_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 147, 1: 6})\n"
     ]
    }
   ],
   "source": [
    "counter = collections.Counter(ranks)\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add doc embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "merged = pd.concat([merged, pd.DataFrame(docvecs)], axis=1)\n",
    "merged = pd.concat([merged, pd.DataFrame(docvecs_avg)], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train & Test Neural Network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_dim = len(mlb.classes_) # number of distinct classes\n",
    "epochs = 25\n",
    "batch_size = 10\n",
    "k_folds = 5 # number of folds for cv\n",
    "pweights = [15, 10, 5, 4, 3, 2, 1]  # multiplier for positive targets, needs to be tuned\n",
    "num_metrics = 5 # number of metrics -- manually set\n",
    "\n",
    "# inputs to loss\n",
    "pos_weight = pweights[0] # starting weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From https://stackoverflow.com/questions/42158866/neural-network-for-multi-label-classification-with-large-number-of-classes-outpu/47313183#47313183\n",
    "def weighted_binary_crossentropy(target, output):\n",
    "    \"\"\"\n",
    "    Weighted binary crossentropy between an output tensor \n",
    "    and a target tensor. POS_WEIGHT is used as a multiplier \n",
    "    for the positive targets.\n",
    "\n",
    "    Combination of the following functions:\n",
    "    * keras.losses.binary_crossentropy\n",
    "    * keras.backend.tensorflow_backend.binary_crossentropy\n",
    "    * tf.nn.weighted_cross_entropy_with_logits\n",
    "    \"\"\"\n",
    "    # transform back to logits\n",
    "    _epsilon = tfb._to_tensor(tfb.epsilon(), output.dtype.base_dtype)\n",
    "    output = tf.clip_by_value(output, _epsilon, 1 - _epsilon)\n",
    "    output = tf.log(output / (1 - output))\n",
    "    # compute weighted loss\n",
    "    loss = tf.nn.weighted_cross_entropy_with_logits(targets=target,\n",
    "                                                    logits=output,\n",
    "                                                    pos_weight=pos_weight)\n",
    "    return tf.reduce_mean(loss, axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that returns the hamming score\n",
    "# correctly predicted / number of labels\n",
    "# Effectively acts as an accuracy metric in multilabel classification\n",
    "def hamming_score(y_true, y_pred):\n",
    "    return (y_pred == y_true).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that given predicted probabilities \n",
    "# and the true labels,\n",
    "# calculates a bunch of metrics and returns them\n",
    "def calc_metrics(y_true, y_prob):\n",
    "    y_pred = np.copy(y_prob) # classes\n",
    "    y_pred[y_pred>=0.5] = 1\n",
    "    y_pred[y_pred<0.5] = 0\n",
    "\n",
    "    # Metrics\n",
    "    # average='micro' because we care a little more about global statistics\n",
    "    ham_score = hamming_score(y_true, y_pred) # accuracy\n",
    "    emr = accuracy_score(y_true, y_pred) # exact match ratio\n",
    "    f1 = f1_score(y_true, y_pred, average='micro') # f1 -- care about false positives and false negatives\n",
    "    prec = precision_score(y_true, y_pred, average='micro') # tp / (tp + fp) # care about false positives slightly more let's look at precision instead of both\n",
    "    auc = roc_auc_score(y_true, y_prob, average='micro') # for ease, pretend the threshold should be the same for all classes\n",
    "    metrics = [ham_score, emr, f1, prec, auc]\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train & test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that runs k_folds cross validation\n",
    "# Note that y, kf, pos_weight, whatever parameters is set outside of this function\n",
    "# Returns the mean metrics\n",
    "def run_cv(x, verbose=True):\n",
    "    cv_scores = np.empty((k_folds, num_metrics))\n",
    "    j = 0\n",
    "    for train_index, test_index in kf.split(x):\n",
    "        if verbose:\n",
    "            print('{:d}th CV run'.format(j))\n",
    "        X_train, X_test = x.iloc[train_index], x.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "        # Build model\n",
    "        model = keras.models.Sequential() \n",
    "        model.add(keras.layers.Dense(128, input_dim=vec_dim, activation='relu'))\n",
    "        model.add(keras.layers.Dense(64, activation='relu'))\n",
    "        model.add(keras.layers.Dense(class_dim, activation='sigmoid')) # sigmoid so that the probability of one class is independent from the probability of another\n",
    "        model.compile(loss=weighted_binary_crossentropy, \n",
    "                  optimizer='adam')\n",
    "        # Fit model\n",
    "        model.fit(X_train, \n",
    "                     y_train,\n",
    "                     epochs=epochs,\n",
    "                     batch_size=batch_size,\n",
    "                     verbose=0)\n",
    "        # Evaluate\n",
    "        y_prob = model.predict(X_test) # probabilities\n",
    "        metrics = calc_metrics(y_test.values, y_prob)\n",
    "        \n",
    "        if verbose:\n",
    "            print('Accuracy: {:.5f}, EMR: {:.5f}, F1: {:.5f}, Precision: {:.5f}, AUC: {:.5f}'.format(*metrics))\n",
    "            \n",
    "        for k in range(num_metrics):\n",
    "            cv_scores[j, k] = metrics[k]\n",
    "            \n",
    "        j = j + 1\n",
    "        \n",
    "        # To prevent slower performance\n",
    "        tfb.clear_session()\n",
    "    return cv_scores.mean(axis=0) # return mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0th CV run\n",
      "WARNING:tensorflow:From C:\\Users\\Phoenix\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\Phoenix\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Accuracy: 0.89606, EMR: 0.19355, F1: 0.53968, Precision: 0.54839, AUC: 0.81734\n",
      "1th CV run\n",
      "Accuracy: 0.89247, EMR: 0.09677, F1: 0.57143, Precision: 0.52632, AUC: 0.86352\n",
      "2th CV run\n",
      "Accuracy: 0.88889, EMR: 0.16129, F1: 0.55072, Precision: 0.46914, AUC: 0.84133\n",
      "3th CV run\n",
      "Accuracy: 0.87593, EMR: 0.13333, F1: 0.51095, Precision: 0.44872, AUC: 0.86042\n",
      "4th CV run\n",
      "Accuracy: 0.90556, EMR: 0.20000, F1: 0.52336, Precision: 0.48276, AUC: 0.86109\n",
      "0th CV run\n",
      "Accuracy: 0.90323, EMR: 0.25806, F1: 0.53448, Precision: 0.52542, AUC: 0.83927\n",
      "1th CV run\n",
      "Accuracy: 0.88889, EMR: 0.16129, F1: 0.49180, Precision: 0.51724, AUC: 0.80870\n",
      "2th CV run\n",
      "Accuracy: 0.90143, EMR: 0.19355, F1: 0.53782, Precision: 0.51613, AUC: 0.86847\n",
      "3th CV run\n",
      "Accuracy: 0.88889, EMR: 0.06667, F1: 0.49153, Precision: 0.43939, AUC: 0.84769\n",
      "4th CV run\n",
      "Accuracy: 0.90185, EMR: 0.26667, F1: 0.55462, Precision: 0.58929, AUC: 0.79981\n",
      "0th CV run\n",
      "Accuracy: 0.90143, EMR: 0.19355, F1: 0.50450, Precision: 0.57143, AUC: 0.85038\n",
      "1th CV run\n",
      "Accuracy: 0.91039, EMR: 0.16129, F1: 0.58333, Precision: 0.62500, AUC: 0.86640\n",
      "2th CV run\n",
      "Accuracy: 0.86918, EMR: 0.09677, F1: 0.37607, Precision: 0.38596, AUC: 0.77095\n",
      "3th CV run\n",
      "Accuracy: 0.91296, EMR: 0.33333, F1: 0.59130, Precision: 0.56667, AUC: 0.90133\n",
      "4th CV run\n",
      "Accuracy: 0.90556, EMR: 0.16667, F1: 0.54054, Precision: 0.50847, AUC: 0.87181\n",
      "0th CV run\n",
      "Accuracy: 0.92294, EMR: 0.16129, F1: 0.62609, Precision: 0.70588, AUC: 0.89610\n",
      "1th CV run\n",
      "Accuracy: 0.91756, EMR: 0.22581, F1: 0.58182, Precision: 0.71111, AUC: 0.86271\n",
      "2th CV run\n",
      "Accuracy: 0.89964, EMR: 0.19355, F1: 0.50000, Precision: 0.51852, AUC: 0.83310\n",
      "3th CV run\n",
      "Accuracy: 0.90185, EMR: 0.16667, F1: 0.52252, Precision: 0.50000, AUC: 0.87881\n",
      "4th CV run\n",
      "Accuracy: 0.91111, EMR: 0.20000, F1: 0.51020, Precision: 0.55556, AUC: 0.81186\n",
      "0th CV run\n",
      "Accuracy: 0.90323, EMR: 0.19355, F1: 0.54237, Precision: 0.60377, AUC: 0.80016\n",
      "1th CV run\n",
      "Accuracy: 0.90143, EMR: 0.19355, F1: 0.51327, Precision: 0.58000, AUC: 0.80888\n",
      "2th CV run\n",
      "Accuracy: 0.90502, EMR: 0.12903, F1: 0.51376, Precision: 0.59574, AUC: 0.79634\n",
      "3th CV run\n",
      "Accuracy: 0.92222, EMR: 0.20000, F1: 0.53333, Precision: 0.58537, AUC: 0.87876\n",
      "4th CV run\n",
      "Accuracy: 0.93148, EMR: 0.33333, F1: 0.63366, Precision: 0.68085, AUC: 0.88083\n",
      "0th CV run\n",
      "Accuracy: 0.90681, EMR: 0.19355, F1: 0.50943, Precision: 0.55102, AUC: 0.85839\n",
      "1th CV run\n",
      "Accuracy: 0.91577, EMR: 0.29032, F1: 0.55238, Precision: 0.64444, AUC: 0.86513\n",
      "2th CV run\n",
      "Accuracy: 0.91398, EMR: 0.25806, F1: 0.54717, Precision: 0.58000, AUC: 0.84181\n",
      "3th CV run\n",
      "Accuracy: 0.92222, EMR: 0.23333, F1: 0.56250, Precision: 0.69231, AUC: 0.87374\n",
      "4th CV run\n",
      "Accuracy: 0.91296, EMR: 0.16667, F1: 0.56881, Precision: 0.67391, AUC: 0.83135\n",
      "0th CV run\n",
      "Accuracy: 0.91219, EMR: 0.16129, F1: 0.49485, Precision: 0.70588, AUC: 0.84804\n",
      "1th CV run\n",
      "Accuracy: 0.89964, EMR: 0.16129, F1: 0.42857, Precision: 0.75000, AUC: 0.82061\n",
      "2th CV run\n",
      "Accuracy: 0.90143, EMR: 0.25806, F1: 0.43299, Precision: 0.53846, AUC: 0.81491\n",
      "3th CV run\n",
      "Accuracy: 0.93519, EMR: 0.26667, F1: 0.61538, Precision: 0.75676, AUC: 0.87776\n",
      "4th CV run\n",
      "Accuracy: 0.91667, EMR: 0.20000, F1: 0.50549, Precision: 0.53488, AUC: 0.89439\n"
     ]
    }
   ],
   "source": [
    "# FIRST: nn model with doc2vec\n",
    "# Split into x and y\n",
    "x_doc = merged.iloc[:, 7+class_dim:7+class_dim+vec_dim]\n",
    "y = merged.iloc[:, 7:7+class_dim]\n",
    "\n",
    "# Cross validation\n",
    "kf = KFold(n_splits=k_folds, shuffle=True)\n",
    "\n",
    "cv_scores_doc = np.empty((len(pweights), num_metrics))\n",
    "for i in range(len(pweights)):\n",
    "    pos_weight = pweights[i]\n",
    "    cv_scores_doc[i] = run_cv(x_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0th CV run\n",
      "Accuracy: 0.89247, EMR: 0.06452, F1: 0.34783, Precision: 0.45714, AUC: 0.76230\n",
      "1th CV run\n",
      "Accuracy: 0.89247, EMR: 0.22581, F1: 0.38776, Precision: 0.44186, AUC: 0.76501\n",
      "2th CV run\n",
      "Accuracy: 0.89964, EMR: 0.19355, F1: 0.44000, Precision: 0.53659, AUC: 0.79216\n",
      "3th CV run\n",
      "Accuracy: 0.88148, EMR: 0.13333, F1: 0.34694, Precision: 0.42500, AUC: 0.76445\n",
      "4th CV run\n",
      "Accuracy: 0.88519, EMR: 0.10000, F1: 0.35417, Precision: 0.53125, AUC: 0.78191\n"
     ]
    }
   ],
   "source": [
    "# SECOND: nn model with averaged word vecs\n",
    "# We do this separately because let's just pick a positive weight and stick to it\n",
    "x_word = merged.iloc[:, 7+class_dim+vec_dim:7+class_dim+vec_dim*2]\n",
    "\n",
    "pos_weight = 2\n",
    "# Now, let's test the model with average vectors\n",
    "cv_scores_word = run_cv(x_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN Model with Doc2Vec Results:\n",
      "[[0.89178017 0.15698925 0.53922983 0.49506305 0.84874182]\n",
      " [0.89685783 0.18924731 0.52204969 0.51749476 0.83278892]\n",
      " [0.89990442 0.19032258 0.51915022 0.53150695 0.85217396]\n",
      " [0.91062127 0.18946237 0.54812635 0.59821351 0.85651671]\n",
      " [0.91267622 0.20989247 0.54728108 0.60914704 0.83299265]\n",
      " [0.91434886 0.2283871  0.54805841 0.62833712 0.85408454]\n",
      " [0.9130227  0.20946237 0.49545712 0.65719687 0.85114295]]\n",
      "\n",
      "NN Model with Averaged Word2Vec Results:\n",
      "[0.8902509  0.14344086 0.37533733 0.47836774 0.77316566]\n"
     ]
    }
   ],
   "source": [
    "print('NN Model with Doc2Vec Results:')\n",
    "print(cv_scores_doc) # each index corresponds to a pos_weight\n",
    "print()\n",
    "print('NN Model with Averaged Word2Vec Results:')\n",
    "print(cv_scores_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
