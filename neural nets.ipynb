{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "unable to import 'smart_open.gcs', disabling that module\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "from nltk.corpus import stopwords # get stopwords to remove\n",
    "import re # regular expression\n",
    "from gensim.models import doc2vec, Word2Vec # for word embeddings\n",
    "from gensim.utils import simple_preprocess # to tokenize automatically\n",
    "from sklearn.model_selection import train_test_split, KFold # for test-train split & cross validation\n",
    "from sklearn.preprocessing import MultiLabelBinarizer # to convert to a format that can do multi-label classification\n",
    "import random\n",
    "import keras # for nn\n",
    "import tensorflow as tf # for nn & new loss\n",
    "import keras.backend.tensorflow_backend as tfb # for nn $ new loss\n",
    "from sklearn.metrics import precision_score, accuracy_score, roc_auc_score, f1_score # for metrics\n",
    "import collections\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "# Ensure reproducibility\n",
    "seed = 561\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "tf.set_random_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "vec_dim = 100 # how big the word embeddings are"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-label classification\n",
    "\n",
    "Classify the reasons (violations) behind each docket_num (document)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text = pd.read_csv('./data/clean_mea_text.csv') # this holds the raw text\n",
    "reasons = pd.read_csv('./data/mea_reasons_filtered.csv') # these are our target classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         date docket_num                                               text  \\\n",
      "0  2009-11-18     09_160  STATE OF NORTH CAROLINA\\nWAKE COUNTY\\nIN A MAT...   \n",
      "1  2009-11-18     09_164  STATE OF NORTH CAROLINA\\nWAKE COUNTY\\nIN A MAT...   \n",
      "2  2009-10-16    09_142B  OAH File No. 10 COB 2895\\nSTATE OF NORTH CAROL...   \n",
      "3  2009-09-09     09_081  STATE OF NORTH CAROLINA\\nWAKE COUNTY\\nIN A MAT...   \n",
      "4  2009-08-24     09_070  STATE OF NORTH CAROLINA\\nWAKE COUNTY\\nIN A MAT...   \n",
      "\n",
      "   year  month  day  \n",
      "0  2009     11   18  \n",
      "1  2009     11   18  \n",
      "2  2009     10   16  \n",
      "3  2009      9    9  \n",
      "4  2009      8   24  \n",
      "(177, 6)\n"
     ]
    }
   ],
   "source": [
    "print(raw_text.head())\n",
    "print(raw_text.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  docket_num        date                                        reason\n",
      "0     09_160  11/18/2009                    Conspiracy to commit fraud\n",
      "1     09_164  11/18/2009                    Conspiracy to commit fraud\n",
      "2    09_142B  10/16/2009                     Allowed unlawful activity\n",
      "3     09_081    9/9/2009  Falsification and misrepresentation of loans\n",
      "4     09_070   8/24/2009                       Retained borrower funds\n",
      "(375, 3)\n"
     ]
    }
   ],
   "source": [
    "print(reasons.head())\n",
    "print(reasons.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(169, 6)\n",
      "(359, 3)\n"
     ]
    }
   ],
   "source": [
    "# There is a bit of data mismatch, so filter both dfs for text that appears in both\n",
    "bothdocs = set(raw_text.docket_num.values).intersection(reasons.docket_num.values)\n",
    "raw_text = raw_text[raw_text.docket_num.isin(bothdocs)]\n",
    "reasons = reasons[reasons.docket_num.isin(bothdocs)]\n",
    "print(raw_text.shape)\n",
    "print(reasons.shape)\n",
    "\n",
    "# Also need to convert datatypes to prevent type mismatch\n",
    "raw_text['text'] = raw_text['text'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove irrelevant/unhelpful labels\n",
    "toremove = ['Unknown', \n",
    "           'Other', \n",
    "           'Did not appear for the Hearing', \n",
    "           'Other failure to disclose', \n",
    "           'Unknown failure to disclose', \n",
    "           'Allowed falsification and misrepresentation of loans',\n",
    "           'Circumvented the requirements that a branch manager of a licensed mortgage broker have at least three years experience',\n",
    "           \"Did not verify or make a reasonable effort to verify the borrower's information\",\n",
    "           'Employed simultaneously by more than one affiliated mortgage banker or mortgage broker',\n",
    "           'Engaged in fraud',\n",
    "           'Failure to disclose charges',\n",
    "           'Violated NC Securities Act',\n",
    "           'Withdrew appeal',\n",
    "           'Unsatisfactory credit']\n",
    "reasons = reasons[~reasons.reason.isin(toremove)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we want to do multi-label classification, binarize outputs\n",
    "# First, need to aggregate reason by docket_num\n",
    "reasonsls = reasons.groupby('docket_num')['reason'].apply(set).reset_index(name='reason')\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "classesbin = mlb.fit_transform(reasonsls.reason.values)\n",
    "classesbin = pd.DataFrame(classesbin)\n",
    "classesbin.columns = mlb.classes_\n",
    "\n",
    "reasonsls = pd.concat([reasonsls, classesbin], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's combine the input and output datasets for easier handling\n",
    "merged = raw_text.merge(reasonsls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(153, 25)\n"
     ]
    }
   ],
   "source": [
    "print(merged.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Params:\n",
    "#    df - dataframe with column 'text' to be tokenized\n",
    "#    tokens_only - to train the doc2vec model, weâ€™ll need to \n",
    "#        associate a tag/number with each document of the training corpus. \n",
    "#        tokens_only=True means don't associate anything\n",
    "def tokenize(df, tokens_only=False):\n",
    "    tokens = df['text'].apply(lambda x: simple_preprocess(x, deacc=True, max_len=20)) # max_len=20 just in case there are important words 15 chars long)\n",
    "    if tokens_only:\n",
    "        return tokens\n",
    "    else:\n",
    "        # For training data, add tags -- notice it is just an index number\n",
    "        return [doc2vec.TaggedDocument(doc, [i]) for i, doc in enumerate(tokens)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorize\n",
    "\n",
    "We use the Continuous Bag of Words (CBOW) model to create our word embeddings to be used in our ML models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Remove extra space\n",
    "merged['text'] = merged['text'].apply(lambda x: re.sub(r'\\s+', ' ', x))\n",
    "# Tokenize\n",
    "corpus = tokenize(merged)\n",
    "\n",
    "docmodel = doc2vec.Doc2Vec(vector_size=vec_dim,min_count=1, epochs=40) # min_count=1 because we're not sure if relevant words occur multiple times\n",
    "docmodel.build_vocab(corpus)\n",
    "# Train\n",
    "docmodel.train(corpus, total_examples=docmodel.corpus_count, epochs=docmodel.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Phoenix\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.vectors instead).\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "# Let's also use word2vec and averaging to compare to doc2vec\n",
    "# Perhaps in this setting, the context of words isn't important\n",
    "textls = tokenize(merged, tokens_only=True)\n",
    "wordmodel = Word2Vec(textls, min_count=1)\n",
    "w2v = dict(zip(wordmodel.wv.index2word, wordmodel.wv.syn0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Params:\n",
    "#    w2v - dictionary of words to vectors\n",
    "#    text - list of tokenized words to convert to vector\n",
    "# Returns a vector resulting from the average of all present words in text\n",
    "def average_vectors(w2v, text):\n",
    "    num_count = collections.Counter(text) # words and their counts\n",
    "    return np.mean([w2v[word]*count for word, count in num_count.items()], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "docvecs_avg = [average_vectors(w2v, corpus[i].words) for i in range(len(corpus))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assess Doc2Vec CBOW model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks = []\n",
    "second_ranks = []\n",
    "docvecs = []\n",
    "for doc_id in range(len(corpus)):\n",
    "    inferred_vector = docmodel.infer_vector(corpus[doc_id].words)\n",
    "    sims = docmodel.docvecs.most_similar([inferred_vector], topn=len(docmodel.docvecs))\n",
    "    rank = [docid for docid, sim in sims].index(doc_id)\n",
    "    ranks.append(rank)\n",
    "\n",
    "    second_ranks.append(sims[1])\n",
    "    \n",
    "    docvecs.append(inferred_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 147, 1: 6})\n"
     ]
    }
   ],
   "source": [
    "counter = collections.Counter(ranks)\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add doc embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "merged = pd.concat([merged, pd.DataFrame(docvecs)], axis=1)\n",
    "merged = pd.concat([merged, pd.DataFrame(docvecs_avg)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['date' 'docket_num' 'text' 'year' 'month' 'day' 'reason'\n",
      " 'Allowed unlawful activity'\n",
      " 'Concealed material facts that were likely to influence a mortgagor to take a mortgage loan'\n",
      " 'Conspiracy to commit fraud' 'Convicted of misdemeanor or felony'\n",
      " 'Does not meet the financial responsibility requirements' 'Excess fees'\n",
      " 'Failure to disclose civil or financial issue'\n",
      " 'Failure to disclose criminal conviction'\n",
      " 'Failure to file a correcting amendment'\n",
      " 'Falsification and misrepresentation of loans'\n",
      " 'Has outstanding tax liens' 'Identity issues'\n",
      " 'Impermissible net-branching' 'Lacks a surety bond'\n",
      " 'Mistreatment of employees' 'Retained borrower funds'\n",
      " 'Uncooperative with OCOB' 'Unlicensed activity' 0 1 2 3 4 5 6 7 8 9 10 11\n",
      " 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35\n",
      " 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59\n",
      " 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83\n",
      " 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 0 1 2 3 4 5 6 7 8 9 10 11\n",
      " 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35\n",
      " 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59\n",
      " 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83\n",
      " 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99]\n"
     ]
    }
   ],
   "source": [
    "print(merged.columns.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train & Test Neural Network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_dim = len(mlb.classes_) # number of distinct classes\n",
    "epochs = 25\n",
    "batch_size = 10\n",
    "k_folds = 5 # number of folds for cv\n",
    "pweights = [15, 10, 5, 4, 3, 2, 1]  # multiplier for positive targets, needs to be tuned\n",
    "\n",
    "pos_weight = pweights[0] # starting weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From https://stackoverflow.com/questions/42158866/neural-network-for-multi-label-classification-with-large-number-of-classes-outpu/47313183#47313183\n",
    "def weighted_binary_crossentropy(target, output):\n",
    "    \"\"\"\n",
    "    Weighted binary crossentropy between an output tensor \n",
    "    and a target tensor. POS_WEIGHT is used as a multiplier \n",
    "    for the positive targets.\n",
    "\n",
    "    Combination of the following functions:\n",
    "    * keras.losses.binary_crossentropy\n",
    "    * keras.backend.tensorflow_backend.binary_crossentropy\n",
    "    * tf.nn.weighted_cross_entropy_with_logits\n",
    "    \"\"\"\n",
    "    # transform back to logits\n",
    "    _epsilon = tfb._to_tensor(tfb.epsilon(), output.dtype.base_dtype)\n",
    "    output = tf.clip_by_value(output, _epsilon, 1 - _epsilon)\n",
    "    output = tf.log(output / (1 - output))\n",
    "    # compute weighted loss\n",
    "    loss = tf.nn.weighted_cross_entropy_with_logits(targets=target,\n",
    "                                                    logits=output,\n",
    "                                                    pos_weight=pos_weight)\n",
    "    return tf.reduce_mean(loss, axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that returns the hamming score\n",
    "# correctly predicted / number of labels\n",
    "# Effectively acts as an accuracy metric in multilabel classification\n",
    "def hamming_score(y_true, y_pred):\n",
    "    return (y_pred == y_true).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that given predicted probabilities \n",
    "# and the true labels,\n",
    "# calculates a bunch of metrics and returns them\n",
    "def calc_metrics(y_true, y_prob):\n",
    "    y_pred = np.copy(y_prob) # classes\n",
    "    y_pred[y_pred>=0.5] = 1\n",
    "    y_pred[y_pred<0.5] = 0\n",
    "\n",
    "    # Metrics\n",
    "    # average='micro' because we care a little more about global statistics\n",
    "    ham_score = hamming_score(y_true, y_pred) # accuracy\n",
    "    emr = accuracy_score(y_true, y_pred) # exact match ratio\n",
    "    f1 = f1_score(y_true, y_pred, average='micro') # f1 -- care about false positives and false negatives\n",
    "    prec = precision_score(y_true, y_pred, average='micro') # tp / (tp + fp) # care about false positives slightly more let's look at precision instead of both\n",
    "    auc = roc_auc_score(y_true, y_prob, average='micro') # for ease, pretend the threshold should be the same for all classes\n",
    "    metrics = [ham_score, emr, f1, prec, auc]\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train & test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that runs k_folds cross validation\n",
    "# Note that x, y, and kf is set outside of this function\n",
    "def run_cv(verbose=True):\n",
    "    cv_scores = np.empty((k_folds, num_metrics))\n",
    "    j = 0\n",
    "    for train_index, test_index in kf.split(x):\n",
    "        if verbose:\n",
    "            print('{:d}th CV run'.format(j))\n",
    "        X_train, X_test = x.iloc[train_index], x.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "        # Build model\n",
    "        model = keras.models.Sequential() \n",
    "        model.add(keras.layers.Dense(128, input_dim=vec_dim, activation='relu'))\n",
    "        model.add(keras.layers.Dense(64, activation='relu'))\n",
    "        model.add(keras.layers.Dense(class_dim, activation='sigmoid')) # sigmoid so that the probability of one class is independent from the probability of another\n",
    "        model.compile(loss=weighted_binary_crossentropy, \n",
    "                  optimizer='adam')\n",
    "        # Fit model\n",
    "        model.fit(X_train, \n",
    "                     y_train,\n",
    "                     epochs=epochs,\n",
    "                     batch_size=batch_size,\n",
    "                     verbose=0)\n",
    "        # Evaluate\n",
    "        y_prob = model.predict(X_test) # probabilities\n",
    "        metrics = calc_metrics(y_test.values, y_prob)\n",
    "        \n",
    "        if verbose:\n",
    "            print(metrics)\n",
    "        \n",
    "        for k in range(num_metrics):\n",
    "            cv_scores[j, k] = metrics[k]\n",
    "            \n",
    "        j = j + 1\n",
    "        \n",
    "        # To prevent slower performance\n",
    "        tfb.clear_session()\n",
    "    return cv_scores.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into x and y\n",
    "x = merged.iloc[:, 7+class_dim:7+class_dim+vec_dim]\n",
    "y = merged.iloc[:, 7:7+class_dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0th CV run\n",
      "WARNING:tensorflow:From C:\\Users\\Phoenix\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\Phoenix\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "[0.899641577060932, 0.16129032258064516, 0.5692307692307692, 0.5606060606060606, 0.8137651821862347]\n",
      "1th CV run\n",
      "[0.8870967741935484, 0.03225806451612903, 0.5401459854014599, 0.5068493150684932, 0.8600075910931173]\n",
      "2th CV run\n",
      "[0.8817204301075269, 0.0967741935483871, 0.547945205479452, 0.449438202247191, 0.8418601393703821]\n",
      "3th CV run\n",
      "[0.8870370370370371, 0.16666666666666666, 0.5196850393700787, 0.4852941176470588, 0.882554001198069]\n",
      "4th CV run\n",
      "[0.9092592592592592, 0.2, 0.5504587155963303, 0.5, 0.8563531318841181]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-1f07b36594c6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpweights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mpos_weight\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpweights\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mcv_scores\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrun_cv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "# Cross validation\n",
    "kf = KFold(n_splits=k_folds, shuffle=True)\n",
    "num_metrics = 5 # number of metrics\n",
    "\n",
    "cv_scores = np.empty((len(pweights),k_folds, num_metrics))\n",
    "for i in range(len(pweights)):\n",
    "    pos_weight = pweights[i]\n",
    "    j = 0 # to index which cv it is\n",
    "    for train_index, test_index in kf.split(x):\n",
    "        print('pos_weight={:d}, {:d}th CV run'.format(pweights[i], j))\n",
    "        X_train, X_test = x.iloc[train_index], x.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "        # Build model\n",
    "        model = keras.models.Sequential() \n",
    "        model.add(keras.layers.Dense(128, input_dim=vec_dim, activation='relu'))\n",
    "        model.add(keras.layers.Dense(64, activation='relu'))\n",
    "        model.add(keras.layers.Dense(class_dim, activation='sigmoid')) # sigmoid so that the probability of one class is independent from the probability of another\n",
    "        model.compile(loss=weighted_binary_crossentropy, \n",
    "                  optimizer='adam')\n",
    "        # Fit model\n",
    "        model.fit(X_train, \n",
    "                     y_train,\n",
    "                     epochs=epochs,\n",
    "                     batch_size=batch_size,\n",
    "                     verbose=0)\n",
    "        \n",
    "        # Evaluate\n",
    "        y_prob = model.predict(X_test) # probabilities\n",
    "        metrics = calc_metrics(y_test.values, y_prob)\n",
    "        \n",
    "        print(metrics)\n",
    "        \n",
    "        for k in range(num_metrics):\n",
    "            cv_scores[i, j, k] = metrics[k]\n",
    "        j = j + 1\n",
    "        \n",
    "        # To prevent slower performance\n",
    "        tfb.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.88677419, 0.12430108, 0.52165945, 0.47706149, 0.84656246],\n",
       "       [0.88882915, 0.17010753, 0.50038028, 0.47980143, 0.83313419],\n",
       "       [0.90169654, 0.19010753, 0.51369912, 0.5431016 , 0.8435737 ],\n",
       "       [0.90663082, 0.18258065, 0.52858373, 0.57419763, 0.85126949],\n",
       "       [0.90424134, 0.18344086, 0.49853778, 0.55688786, 0.82617351],\n",
       "       [0.91290323, 0.24150538, 0.53341491, 0.62330962, 0.85674799],\n",
       "       [0.90357228, 0.20301075, 0.44538039, 0.57839154, 0.84580122]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_scores.mean(axis=1) # find average for each metric by pos_weight -- each index now corresponds to a pos_weight\n",
    "# array([[0.88677419, 0.12430108, 0.52165945, 0.47706149, 0.84656246],\n",
    "#        [0.88882915, 0.17010753, 0.50038028, 0.47980143, 0.83313419],\n",
    "#        [0.90169654, 0.19010753, 0.51369912, 0.5431016 , 0.8435737 ],\n",
    "#        [0.90663082, 0.18258065, 0.52858373, 0.57419763, 0.85126949],\n",
    "#        [0.90424134, 0.18344086, 0.49853778, 0.55688786, 0.82617351],\n",
    "#        [0.91290323, 0.24150538, 0.53341491, 0.62330962, 0.85674799],\n",
    "#        [0.90357228, 0.20301075, 0.44538039, 0.57839154, 0.84580122]])\n",
    "# best pos_weight = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0th CV run\n",
      "[0.9014336917562724, 0.06451612903225806, 0.4086021505376344, 0.5277777777777778, 0.7576426095178065]\n",
      "1th CV run\n",
      "[0.9014336917562724, 0.22580645161290322, 0.4329896907216495, 0.5, 0.7598409542743539]\n",
      "2th CV run\n",
      "[0.9014336917562724, 0.16129032258064516, 0.4444444444444445, 0.55, 0.7831255731802589]\n",
      "3th CV run\n",
      "[0.8944444444444445, 0.1, 0.3736263736263736, 0.5151515151515151, 0.7664901988839605]\n",
      "4th CV run\n",
      "[0.8814814814814815, 0.1, 0.3333333333333333, 0.5, 0.796875]\n"
     ]
    }
   ],
   "source": [
    "x = merged.iloc[:, 7+class_dim+vec_dim:7+class_dim+vec_dim*2]\n",
    "\n",
    "pos_weight = 2\n",
    "cv_scores2 = np.empty((k_folds, num_metrics))\n",
    "# Now, let's test the model with average vectors\n",
    "j=0\n",
    "for train_index, test_index in kf.split(x):\n",
    "    print('{:d}th CV run'.format(j))\n",
    "    X_train, X_test = x.iloc[train_index], x.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    # Build model\n",
    "    model = keras.models.Sequential() \n",
    "    model.add(keras.layers.Dense(128, input_dim=vec_dim, activation='relu'))\n",
    "    model.add(keras.layers.Dense(64, activation='relu'))\n",
    "    model.add(keras.layers.Dense(class_dim, activation='sigmoid')) # sigmoid so that the probability of one class is independent from the probability of another\n",
    "    model.compile(loss=weighted_binary_crossentropy, \n",
    "              optimizer='adam')\n",
    "    # Fit model\n",
    "    model.fit(X_train, \n",
    "                 y_train,\n",
    "                 epochs=epochs,\n",
    "                 batch_size=batch_size,\n",
    "                 verbose=0)\n",
    "\n",
    "    # Evaluate\n",
    "    y_prob = model.predict(X_test) # probabilities\n",
    "    metrics = calc_metrics(y_test.values, y_prob)\n",
    "\n",
    "    print(metrics)\n",
    "\n",
    "    for k in range(num_metrics):\n",
    "        cv_scores2[j, k] = metrics[k]\n",
    "    j = j + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.8960454 , 0.13032258, 0.3985992 , 0.51858586, 0.77279487])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_scores2.mean(axis=0) \n",
    "# array([0.8960454 , 0.13032258, 0.3985992 , 0.51858586, 0.77279487])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
