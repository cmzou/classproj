{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "from nltk.corpus import stopwords # get stopwords to remove\n",
    "import re # regular expression\n",
    "import string # used to remove punctuation\n",
    "from gensim.models import Word2Vec # for word embeddings\n",
    "stop = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-label classification\n",
    "\n",
    "Classify the reasons (violations) behind each docket_num (document)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text = pd.read_csv('./data/clean_mea_text.csv') # this holds the raw text\n",
    "reasons = pd.read_csv('./data/mea_reasons_filtered.csv') # these are our target classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         date docket_num                                               text  \\\n",
      "0  2009-11-18     09_160  STATE OF NORTH CAROLINA\\nWAKE COUNTY\\nIN A MAT...   \n",
      "1  2009-11-18     09_164  STATE OF NORTH CAROLINA\\nWAKE COUNTY\\nIN A MAT...   \n",
      "2  2009-10-16    09_142B  OAH File No. 10 COB 2895\\nSTATE OF NORTH CAROL...   \n",
      "3  2009-09-09     09_081  STATE OF NORTH CAROLINA\\nWAKE COUNTY\\nIN A MAT...   \n",
      "4  2009-08-24     09_070  STATE OF NORTH CAROLINA\\nWAKE COUNTY\\nIN A MAT...   \n",
      "\n",
      "   year  month  day  \n",
      "0  2009     11   18  \n",
      "1  2009     11   18  \n",
      "2  2009     10   16  \n",
      "3  2009      9    9  \n",
      "4  2009      8   24  \n",
      "(177, 6)\n"
     ]
    }
   ],
   "source": [
    "print(raw_text.head())\n",
    "print(raw_text.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  docket_num        date                                        reason\n",
      "0     09_160  11/18/2009                    Conspiracy to commit fraud\n",
      "1     09_164  11/18/2009                    Conspiracy to commit fraud\n",
      "2    09_142B  10/16/2009                     Allowed unlawful activity\n",
      "3     09_081    9/9/2009  Falsification and misrepresentation of loans\n",
      "4     09_070   8/24/2009                       Retained borrower funds\n",
      "(375, 3)\n"
     ]
    }
   ],
   "source": [
    "print(reasons.head())\n",
    "print(reasons.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(169, 6)\n",
      "(359, 3)\n"
     ]
    }
   ],
   "source": [
    "# There is a bit of data mismatch, so filter both dfs for text that appears in both\n",
    "bothdocs = set(raw_text.docket_num.values).intersection(reasons.docket_num.values)\n",
    "raw_text = raw_text[raw_text.docket_num.isin(bothdocs)]\n",
    "reasons = reasons[reasons.docket_num.isin(bothdocs)]\n",
    "print(raw_text.shape)\n",
    "print(reasons.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "raw_text['text'] = raw_text['text'].astype(str)\n",
    "# Lowercase\n",
    "raw_text['text'] = raw_text['text'].str.lower()\n",
    "# Remove extra space\n",
    "raw_text['text'] = raw_text['text'].apply(lambda x: re.sub(r'\\s+', ' ', x))\n",
    "# Don't need numbers or punctuation\n",
    "raw_text['text'] = raw_text['text'].apply(lambda x: re.sub(r'\\d+', ' ', x))\n",
    "raw_text['text'] = raw_text['text'].apply(lambda x: x.translate(str.maketrans('', '', string.punctuation)))\n",
    "# Remove extra space again in case punctuation created more space \n",
    "# (and also need to do before and after since removing punctuation removes /)\n",
    "raw_text['text'] = raw_text['text'].apply(lambda x: re.sub(r'\\s+', ' ', x))\n",
    "# Remove stop words - don't do if using CBOW since context of word is important\n",
    "# raw_text['text'] = raw_text['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>docket_num</th>\n",
       "      <th>text</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2009-11-18</td>\n",
       "      <td>09_160</td>\n",
       "      <td>state of north carolina wake county in a matte...</td>\n",
       "      <td>2009</td>\n",
       "      <td>11</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2009-11-18</td>\n",
       "      <td>09_164</td>\n",
       "      <td>state of north carolina wake county in a matte...</td>\n",
       "      <td>2009</td>\n",
       "      <td>11</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2009-10-16</td>\n",
       "      <td>09_142B</td>\n",
       "      <td>oah file no cob state of north carolina eee wa...</td>\n",
       "      <td>2009</td>\n",
       "      <td>10</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2009-09-09</td>\n",
       "      <td>09_081</td>\n",
       "      <td>state of north carolina wake county in a matte...</td>\n",
       "      <td>2009</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2009-08-24</td>\n",
       "      <td>09_070</td>\n",
       "      <td>state of north carolina wake county in a matte...</td>\n",
       "      <td>2009</td>\n",
       "      <td>8</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date docket_num                                               text  \\\n",
       "0  2009-11-18     09_160  state of north carolina wake county in a matte...   \n",
       "1  2009-11-18     09_164  state of north carolina wake county in a matte...   \n",
       "2  2009-10-16    09_142B  oah file no cob state of north carolina eee wa...   \n",
       "3  2009-09-09     09_081  state of north carolina wake county in a matte...   \n",
       "4  2009-08-24     09_070  state of north carolina wake county in a matte...   \n",
       "\n",
       "   year  month  day  \n",
       "0  2009     11   18  \n",
       "1  2009     11   18  \n",
       "2  2009     10   16  \n",
       "3  2009      9    9  \n",
       "4  2009      8   24  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_text.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorize\n",
    "\n",
    "We use the Continuous Bag of Words model to create our word embeddings to be used in our ML models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Phoenix\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.vectors instead).\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "# Need list of lists as input to gensim\n",
    "textls = [t.split(' ') for t in raw_text.text]\n",
    "wordmodel = Word2Vec(textls, min_count=1) # min_count=1 because we're not sure if relevant words occur multiple times\n",
    "embedls = wordmodel.wv.syn0 # our word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
